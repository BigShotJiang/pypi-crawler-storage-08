{
  "name": "vllm-fixed",
  "version": "0.8.5.post1",
  "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "author": "vLLM Team",
  "license": "Apache-2.0",
  "home_page": null,
  "download_filename": "vllm_fixed-0.8.5.post1.tar.gz",
  "download_time": "2025-10-08T18:52:52.349214",
  "package_url": "https://pypi.org/project/vllm-fixed/"
}