{
  "name": "vllm-fixed",
  "version": "1.0.0",
  "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "author": "vLLM Team",
  "license": "Apache-2.0",
  "home_page": null,
  "download_filename": "vllm_fixed-1.0.0.tar.gz",
  "download_time": "2025-10-08T19:40:55.888469",
  "package_url": "https://pypi.org/project/vllm-fixed/"
}